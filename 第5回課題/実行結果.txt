% python sample_code.py
train loss:2.2999378924737246
=== epoch:1, train acc:0.249, test acc:0.258 ===
train loss:2.296456167608862
train loss:2.2934642201894566
train loss:2.285988783918287
train loss:2.2785035296910308
train loss:2.268883551397144
train loss:2.2560094819120393
train loss:2.2389618792771135
train loss:2.213057097109782
train loss:2.1924058605826087
train loss:2.1551471622061196
train loss:2.144215893224294
train loss:2.0734520008972033
train loss:1.9763951363578645
train loss:1.9376763331422004
train loss:1.94058622163059
train loss:1.8060645645787898
train loss:1.8099403447038989
train loss:1.6489541618252286
train loss:1.6215927704646134
train loss:1.5813528025896577
train loss:1.5054303972782284
train loss:1.2915418699758185
train loss:1.338876426469097
train loss:1.1944582480641281
train loss:1.1089163049611808
train loss:1.1239709394481172
train loss:1.0119385569968322
train loss:0.8415430991642792
train loss:0.8252732683230688
train loss:0.8948460514752199
train loss:0.6923957641774884
train loss:0.7010595381841159
train loss:0.7738588437486638
train loss:0.6975877523410358
train loss:0.7842979317802411
train loss:0.6110930445562375
train loss:0.7601674171998901
train loss:0.7037892643798476
train loss:0.5562634184950026
train loss:0.562330140488557
train loss:0.6572258324783511
train loss:0.5015102761999578
train loss:0.4783460598835118
train loss:0.4956347116083964
train loss:0.5142338643779195
train loss:0.5005902573044262
train loss:0.3923239130326642
train loss:0.46713340728746977
train loss:0.5557446122436962
train loss:0.5767731365483587
=== epoch:2, train acc:0.821, test acc:0.797 ===
train loss:0.6162903841084324
train loss:0.3935075080326626
train loss:0.44309588401575034
train loss:0.511776102128852
train loss:0.5639772264005137
train loss:0.4907383818016797
train loss:0.4937720139212137
train loss:0.41500405276474356
train loss:0.3975606264636802
train loss:0.5794591844609672
train loss:0.45841200637474644
train loss:0.42099275242658973
train loss:0.4725243897479863
train loss:0.5103586144926515
train loss:0.4548037047414257
train loss:0.29212990339578626
train loss:0.3938330664805372
train loss:0.3630079837451059
train loss:0.362908472812882
train loss:0.4928668725494083
train loss:0.5397123862931303
train loss:0.38566020968042525
train loss:0.385760209992999
train loss:0.48042700194434707
train loss:0.4662817598927416
train loss:0.47549194305348147
train loss:0.33624890179222255
train loss:0.3356733903117008
train loss:0.3995603792161191
train loss:0.2402939949496276
train loss:0.6069413104835397
train loss:0.26362203921017424
train loss:0.6146214998641376
train loss:0.36907183953029005
train loss:0.3481282410147799
train loss:0.281480775262235
train loss:0.36478091000492635
train loss:0.3488073095271453
train loss:0.3430128841892928
train loss:0.30101363466097764
train loss:0.2782524516580661
train loss:0.24488831128681818
train loss:0.25542546827441787
train loss:0.2981475836349894
train loss:0.29939215446178374
train loss:0.2833334697620183
train loss:0.5097879812643634
train loss:0.4758219631545489
train loss:0.22195329041841266
train loss:0.33899494682479386
=== epoch:3, train acc:0.872, test acc:0.87 ===
train loss:0.3233500259479047
train loss:0.2381148416645301
train loss:0.34897190483387613
train loss:0.4032129118212462
train loss:0.38231366961867663
train loss:0.29530897346144674
train loss:0.2415512903208016
train loss:0.35006317285531
train loss:0.3161875017650132
train loss:0.42650950633639717
train loss:0.3011783730536082
train loss:0.26504106798552957
train loss:0.24940360240302253
train loss:0.2426980284956098
train loss:0.3610990602859998
train loss:0.279286525247739
train loss:0.340034983751427
train loss:0.3795160850620099
train loss:0.3763080262594045
train loss:0.3704089709521671
train loss:0.2671469445337795
train loss:0.11910641089532367
train loss:0.5217472759722122
train loss:0.2512663870346408
train loss:0.25091290782109743
train loss:0.29876739879036607
train loss:0.20175847683684384
train loss:0.2719820436979686
train loss:0.2051278907369884
train loss:0.2492135220030523
train loss:0.2882069563740836
train loss:0.37553370640886163
train loss:0.28185492162263737
train loss:0.30424510844171093
train loss:0.33267379607510456
train loss:0.26890110242783644
train loss:0.2548184430143723
train loss:0.3606836232688667
train loss:0.24384802233339362
train loss:0.26216278123437387
train loss:0.29636152097582447
train loss:0.26105911971230567
train loss:0.2968223176068495
train loss:0.5483608378276502
train loss:0.3174475836060667
train loss:0.24124283019463522
train loss:0.30007973566260904
train loss:0.275174935042765
train loss:0.35311782341366027
train loss:0.3006833236135782
=== epoch:4, train acc:0.888, test acc:0.872 ===
train loss:0.37885087224402075
train loss:0.34821615881829887
train loss:0.3498923534599442
train loss:0.31422515808699175
train loss:0.29218090596326574
train loss:0.2280827610951043
train loss:0.4152131074457408
train loss:0.3619389540562039
train loss:0.3478097640519537
train loss:0.2730140506835974
train loss:0.2241226359796386
train loss:0.3666098860690779
train loss:0.2504307652099433
train loss:0.31662101820799426
train loss:0.29669987283941074
train loss:0.26127220988424793
train loss:0.2722775860392233
train loss:0.3431724642693203
train loss:0.23641972579837658
train loss:0.34527504369369383
train loss:0.3838275371683145
train loss:0.30020288642270326
train loss:0.43450298237568313
train loss:0.20318246287985298
train loss:0.23399856311867637
train loss:0.2808944912674772
train loss:0.2333248690136687
train loss:0.3608707316336034
train loss:0.27311035229377867
train loss:0.3722987688157115
train loss:0.3009636039040365
train loss:0.20327450639064765
train loss:0.46510659293696166
train loss:0.2949447572883013
train loss:0.5573355195680783
train loss:0.17171161504493476
train loss:0.2313663819107396
train loss:0.2657756920469021
train loss:0.1726128099951176
train loss:0.3455260593226759
train loss:0.29424857174815605
train loss:0.17727962671985784
train loss:0.2268204400165985
train loss:0.25774654320093116
train loss:0.2861276880406278
train loss:0.20488856810299358
train loss:0.2327526729545924
train loss:0.3359344854307175
train loss:0.16909775454786996
train loss:0.2772173704100288
=== epoch:5, train acc:0.906, test acc:0.89 ===
train loss:0.31183427187981283
train loss:0.18123179529696254
train loss:0.3114233456785977
train loss:0.24813789721678958
train loss:0.15959699327053803
train loss:0.3702040523816628
train loss:0.19846458788117233
train loss:0.16380961891318616
train loss:0.1410772630767274
train loss:0.14796940539754538
train loss:0.19255413384181022
train loss:0.2820991364466682
train loss:0.34216284563632887
train loss:0.2132027646705537
train loss:0.27730848416629306
train loss:0.3190702800378368
train loss:0.1552298967055024
train loss:0.22989489960984025
train loss:0.16235258010239406
train loss:0.18511676468322694
train loss:0.13787969929858154
train loss:0.1940778154436494
train loss:0.29704194768412845
train loss:0.31405134418763764
train loss:0.2588257319596117
train loss:0.16326676738759144
train loss:0.20846373014726552
train loss:0.2644744173116451
train loss:0.11801585679015285
train loss:0.3081025066843724
train loss:0.15979075792622516
train loss:0.23385491939166583
train loss:0.13066474095090486
train loss:0.17410558282898583
train loss:0.1868981664242733
train loss:0.17388923524237257
train loss:0.20598373569284237
train loss:0.22892466766694766
train loss:0.11797354430451609
train loss:0.2508336065022957
train loss:0.2633031300962037
train loss:0.14262969468513154
train loss:0.11603852004033163
train loss:0.1695544199587774
train loss:0.21119879241821038
train loss:0.2806268451295531
train loss:0.20913478775370287
train loss:0.4779521538601035
train loss:0.10361697281980342
=============== Final Test Accuracy ===============
test acc:0.909
Saved Network Parameters!
